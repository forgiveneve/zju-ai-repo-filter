
import streamlit as st
import requests
import pandas as pd
import time
from datetime import datetime

st.set_page_config(page_title="GitHub AI é¡¹ç›®ç­›é€‰å™¨", layout="wide")
st.title("ğŸ” GitHub AI åè®­ç»ƒé¡¹ç›®ç­›é€‰å™¨")

keywords = [
    "LoRA", "SFT", "RLHF", "transformer", "BERT", "Chatbot", "Prompt", "LLM",
    "GPT", "finetune", "tuning", "adapter", "knowledge distillation", "self-instruct"
]
min_stars = st.slider("æœ€å° Stars", 0, 500, 2)
max_repos = st.slider("æœ€å¤šç»“æœæ•°", 100, 3000, 1000)
github_token = st.text_input("GitHub Tokenï¼ˆå¿…å¡«ï¼‰", type="password")

created_ranges = [
    ("2023-01-01", "2023-06-01"),
    ("2023-06-01", "2023-09-01"),
    ("2023-09-01", "2024-01-01"),
    ("2024-01-01", "2024-06-01"),
    ("2024-06-01", datetime.today().strftime("%Y-%m-%d"))
]

def search_github_repos(keyword, page, headers, min_stars, start_date, end_date):
    url = "https://api.github.com/search/repositories"
    query = f"{keyword} language:Python stars:>={min_stars} created:{start_date}..{end_date}"
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc",
        "per_page": 30,
        "page": page
    }
    response = requests.get(url, headers=headers, params=params)
    if response.status_code == 200:
        return response.json()
    else:
        return {}

def get_user_info(username, headers):
    url = f"https://api.github.com/users/{username}"
    res = requests.get(url, headers=headers)
    if res.status_code == 200:
        user = res.json()
        return user.get("email"), user.get("bio")
    return None, None

if st.button("å¼€å§‹ç­›é€‰"):
    if not github_token:
        st.error("è¯·æä¾› GitHub Tokenã€‚")
    else:
        headers = {"Authorization": f"token {github_token}"}
        all_results = []
        st.write(f"æ­£åœ¨å¤„ç†å…³é”®è¯: {keywords_input}")
        with st.spinner("æ­£åœ¨æŠ“å– GitHub æ•°æ®ï¼Œè¯·ç¨å€™..."):
            for keyword in [k.strip() for k in keywords_input.split(",") if k.strip()]:
                for start_date, end_date in created_ranges:
                    for page in range(1, 35):  # æ¯æ®µæœ€å¤šæŠ“ 34 é¡µï¼ˆ1020 æ¡ï¼‰
                        data = search_github_repos(keyword, page, headers, min_stars, start_date, end_date)
                        items = data.get("items", [])
                        if not items:
                            break
                        for item in items:
                            username = item["owner"]["login"]
                            email, bio = get_user_info(username, headers)
                            all_results.append({
                                "é¡¹ç›®åç§°": item["name"],
                                "æè¿°": item["description"],
                                "Stars": item["stargazers_count"],
                                "é“¾æ¥": item["html_url"],
                                "ä½œè€…": username,
                                "é‚®ç®±": email,
                                "Bio": bio,
                                "æ˜¯å¦å¯èƒ½æ¥è‡ªZJU": "Zhejiang" in (bio or "") or "ZJU" in (bio or "") or "æµ™æ±Ÿå¤§å­¦" in (bio or "") or "æµ™å¤§" in (bio or ""),
                                "æ˜¯å¦ZJUé‚®ç®±": email.endswith("zju.edu.cn") if email else False
                            })
                            if len(all_results) >= max_repos:
                                break
                        time.sleep(1)
                    if len(all_results) >= max_repos:
                        break
                if len(all_results) >= max_repos:
                    break
        df = pd.DataFrame(all_results)
        st.success(f"å…±è·å– {len(df)} ä¸ªé¡¹ç›®")
        st.dataframe(df)
        csv = df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½CSVæ–‡ä»¶", csv, "github_ai_candidates.csv", "text/csv")
